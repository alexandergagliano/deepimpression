{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0645514e-8752-401b-b0e2-5378b750fc8e",
   "metadata": {},
   "source": [
    "## 08/11/2023\n",
    "\n",
    "Transformer taken from:\n",
    "https://keras.io/examples/timeseries/timeseries_transformer_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cd7a1-be55-4a5c-8eb8-a9a90c9aa9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from contextlib import redirect_stdout\n",
    "from tensorflow.keras.callbacks import (\n",
    "    CSVLogger,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    ")\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "from astronet.t2.model import T2Model\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1812b-815e-4fac-99ed-f8dbe8549bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593a66b-45cb-4396-b4f6-aae2d5fb5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with my own data \n",
    "#only using the 100GP model so that we don't have to use padding, otherwise we would use 0pt2GP.\n",
    "dir = '/Users/alexgagliano/Documents/Research/HostClassifier/transformer/ZTF_Data/'\n",
    "X_train = np.load(dir + 'X_train_ZTF_Sim_FullSliced_Padded_100GP_hostPhotTrue_30Cut.npz')['arr_0']\n",
    "X_test = np.load(dir + 'X_test_ZTF_Sim_FullSliced_Padded_100GP_hostPhotTrue_30Cut.npz')['arr_0']\n",
    "y_train = np.load(dir + 'y_train_ZTF_Sim_FullSliced_Padded_100GP_hostPhotTrue_30Cut.npz')['arr_0']\n",
    "y_test = np.load(dir + 'y_test_ZTF_Sim_FullSliced_Padded_100GP_hostPhotTrue_30Cut.npz')['arr_0']\n",
    "\n",
    "params = {}\n",
    "params['class_weight'] = {0:1, 1:1, 2:1} #not weighting the classes\n",
    "\n",
    "#only weighting in time \n",
    "weights = np.ones(np.shape(X_train[:, :, 0]))\n",
    "weights[(X_train[:, :, 0][:, -1] < 3)] = 10\n",
    "weights[(X_train[:, :, 0][:, -1] > 3) & (X_train[:, :, 0][:, -1] < 15)] = 5\n",
    "\n",
    "weights[y_train == 0] *= params['class_weight'][0]\n",
    "weights[y_train == 1] *= params['class_weight'][1]\n",
    "weights[y_train == 2] *= params['class_weight'][2]\n",
    "\n",
    "#compress -- not doing in time anymore\n",
    "weights = weights[:, 0]\n",
    "\n",
    "#remove the fits with only 4 or fewer datapoints\n",
    "#x_train_GP = [X_train[x, :, 0][5] != 0 for x in np.arange(len(X_train))]\n",
    "#x_test_GP = [X_test[x, :, 0][5] != 0 for x in np.arange(len(X_test))]\n",
    "\n",
    "#X_train = X_train[x_train_GP]\n",
    "#y_train = y_train[x_train_GP]\n",
    "#X_test = X_test[x_test_GP]\n",
    "#y_test = y_test[x_test_GP]\n",
    "\n",
    "#randomly grab 20% of train and test sets \n",
    "subset_frac = 0.5\n",
    "idx_train = np.random.choice(np.arange(len(X_train)), size =int(subset_frac*len(X_train)), replace=False)\n",
    "idx_test = np.random.choice(np.arange(len(X_test)), size =int(subset_frac*len(X_test)), replace=False)\n",
    "\n",
    "X_train = X_train[idx_train]\n",
    "y_train = y_train[idx_train]\n",
    "X_test = X_test[idx_test]\n",
    "y_test = y_test[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a2e62-4635-436d-88f9-9dc481bf3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarize labels \n",
    "y_train = OneHotEncoder(max_categories=3, sparse_output=False).fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = OneHotEncoder(max_categories=3, sparse_output=False).fit_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "def objective(trial):\n",
    "    # Clear clutter from previous Keras session graphs.\n",
    "    clear_session()\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 10\n",
    "    \n",
    "    print(type(X_train))\n",
    "\n",
    "    outputfn = '100GP_Transformer_FullSimTraining_FullOpt'\n",
    "\n",
    "\n",
    "    filters=trial.suggest_int(\"filters\", 32, 64),\n",
    "    ff_dim = trial.suggest_int(\"hidden_layer_size\", 32, 256)\n",
    "    embed_dim = trial.suggest_int(\"embed_dim\", 32, 256, step=32) #should be divisible by the number of heads\n",
    "    num_layers = trial.suggest_categorical(\"num_layers\", [2, 4, 8])\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [2, 4, 8])\n",
    "    droprate = trial.suggest_float(\"droprate\", 0.2, 0.8)\n",
    "     \n",
    "    #embed_dim = 64  # --> Embedding size for each token\n",
    "    #num_heads = 4  # --> Number of attention heads\n",
    "    #ff_dim = 128  # --> Hidden layer size in feed forward network inside transformer\n",
    "    \n",
    "    # --> Number of filters to use in ConvEmbedding block, should be equal to embed_dim\n",
    "    num_filters = embed_dim\n",
    "    \n",
    "    #num_layers = 4  # --> N x repeated transformer blocks\n",
    "    #droprate = 0.3  # --> Rate of neurons to drop\n",
    "    passbands = 'XY'\n",
    "    ts = int(time.time())\n",
    "    outputdir = '/Users/alexgagliano/Documents/Research/HostClassifier/transformer/'\n",
    "    textPath = outputdir + '/text/'\n",
    "    #print(ff_dim)\n",
    "    \n",
    "    (\n",
    "        _,\n",
    "        timesteps,\n",
    "        num_features,\n",
    "    ) = X_train.shape  # X_train.shape[1:] == (TIMESTEPS, num_features)\n",
    "    input_shape = (BATCH_SIZE, timesteps, num_features)\n",
    "    \n",
    "    model = T2Model(\n",
    "        input_dim=input_shape,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        ff_dim=ff_dim,\n",
    "        num_filters=num_filters,\n",
    "        num_classes=num_classes,\n",
    "        num_layers=num_layers,\n",
    "        droprate=droprate,\n",
    "    )\n",
    "\n",
    "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=1.e-3)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"acc\"])\n",
    "    \n",
    "    mc = ModelCheckpoint(outputdir+'/models/Model_%s_CheckpointWeights.sav'%outputfn, monitor='val_loss', mode='min', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "    es = EarlyStopping(min_delta=0.001, mode=\"min\", monitor=\"val_loss\", patience=20, restore_best_weights=True,verbose=1)\n",
    "    #rlrop = ReduceLROnPlateau(cooldown=5, factor=0.1, mode=\"min\", monitor=\"val_loss\", patience=5, verbose=1)\n",
    "\n",
    "    _ = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[mc, es], verbose=2)\n",
    "    \n",
    "    model.build_graph(input_shape)\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    # Evaluate the model accuracy on the validation set.\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    #returns the loss - switch to score[1] to return the accuracy instead\n",
    "    return score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a818e22-24b9-42c5-ab5f-8ca993b07d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sampler = TPESampler(seed=325)  # Make the sampler behave in a deterministic way.\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3bbeea-eaad-421c-9cf4-865619ee66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(outputdir+'/models/Model_%s_CheckpointWeights.sav'%outputfn)\n",
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ab65d-8676-4435-abab-4326badfff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ef40d-0f11-417a-8d9a-19ef75af27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the results as a function of phase \n",
    "y_test = y_test.argmax(axis=1)\n",
    "\n",
    "X_test_first3 = X_test[np.nanmax(X_test[:, :, 0], axis=1) < 3, :, :]\n",
    "y_test_first3 = y_test[np.nanmax(X_test[:, :, 0], axis=1) < 3]\n",
    "\n",
    "X_test_mid15 = X_test[(np.nanmax(X_test[:, :, 0], axis=1) >3) & (np.nanmax(X_test[:, :, 0], axis=1) < 15), :, :]\n",
    "y_test_mid15 = y_test[(np.nanmax(X_test[:, :, 0], axis=1) >3) & (np.nanmax(X_test[:, :, 0], axis=1) < 15)]\n",
    "\n",
    "\n",
    "X_test_last15 = X_test[np.nanmax(X_test[:, :, 0], axis=1) > 15, :, :]\n",
    "y_test_last15 = y_test[np.nanmax(X_test[:, :, 0], axis=1) > 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a17d2b-8d29-4591-884f-c903090df1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### calculating statistics ######################\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "for X, y in [(X_test_first3, y_test_first3), (X_test_mid15, y_test_mid15), (X_test_last15, y_test_last15)]:\n",
    "    f1 = []\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    for train,test in cv.split(X, y):\n",
    "        f1.append(sklearn.metrics.f1_score(y[test],  np.argmax(model.predict(X[test]), axis=1), average='macro'))\n",
    "    nanmed = np.nanmedian(f1)\n",
    "    nanstd = np.nanstd(f1)\n",
    "    print(\"f1: %.2f +/- %.2f\"%(nanmed, nanstd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d7016-0aa2-49a0-be0c-9b0da0a2c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced auroc score\n",
    "for X, y in [(X_test_first3, y_test_first3), (X_test_mid15, y_test_mid15), (X_test_last15, y_test_last15)]:\n",
    "    auroc = []\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    for train,test in cv.split(X, y):\n",
    "        auroc.append(sklearn.metrics.roc_auc_score(y[test],  model.predict(X[test]), average='macro', multi_class='ovr'))\n",
    "    print(np.nanmedian(auroc))\n",
    "    print(np.nanstd(auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ab2bd-6d70-405f-982c-311ebf2a00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced precision:\n",
    "for X, y in [(X_test_first3, y_test_first3), (X_test_mid15, y_test_mid15), (X_test_last15, y_test_last15)]:\n",
    "    prec = []\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    for train,test in cv.split(X, y):\n",
    "        prec.append(sklearn.metrics.precision_score(y[test],  np.argmax(model.predict(X[test]), axis=1), average='macro'))\n",
    "    print(np.nanmedian(prec))\n",
    "    print(np.nanstd(prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e3035-50f9-4891-bf28-099d21ccdb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall\n",
    "for X, y in [(X_test_first3, y_test_first3), (X_test_mid15, y_test_mid15), (X_test_last15, y_test_last15)]:\n",
    "    rec = []\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    for train,test in cv.split(X, y):\n",
    "        rec.append(sklearn.metrics.recall_score(y,  np.argmax(model.predict(X), axis=1), average='macro'))\n",
    "    print(np.nanmedian(rec))\n",
    "    print(np.nanstd(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa95e1-f173-4779-8022-5f3bb6c4e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced accuracy:\n",
    "for X, y in [(X_test_first3, y_test_first3), (X_test_mid15, y_test_mid15), (X_test_last15, y_test_last15)]:\n",
    "    bacc = []\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    for train,test in cv.split(X, y):\n",
    "        bacc.append(sklearn.metrics.balanced_accuracy_score(y[test],  np.argmax(model.predict(X[test]), axis=1)))\n",
    "    print(np.nanmedian(bacc))\n",
    "    print(np.nanstd(bacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba8c91-d2bc-4a3f-a952-7c7a8af4ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in [(X_test_first3, y_test_first3), (X_test_mid15, y_test_mid15), (X_test_last15, y_test_last15)]:\n",
    "    acc = []\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "    for train,test in cv.split(X, y):\n",
    "        acc.append(sklearn.metrics.accuracy_score(y[test],  np.argmax(model.predict(X[test]), axis=1)))\n",
    "    print(np.nanmedian(acc))\n",
    "    print(np.nanstd(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c59657-e046-46b0-9527-2e8f69acd6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PR_wCV(model, params, ax, X, y, encoding_dict, fnstr='', plotpath='./', save=True):\n",
    "    \"\"\"Short summary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : type\n",
    "        Description of parameter `model`.\n",
    "    ax : type\n",
    "        Description of parameter `ax`.\n",
    "    X : type\n",
    "        Description of parameter `X`.\n",
    "    y : type\n",
    "        Description of parameter `y`.\n",
    "    encoding_dict : type\n",
    "        Description of parameter `encoding_dict`.\n",
    "    fnstr : type\n",
    "        Description of parameter `fnstr`.\n",
    "    save : type\n",
    "        Description of parameter `save`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    type\n",
    "        Description of returned object.\n",
    "\n",
    "    \"\"\"\n",
    "    nsplit = 5\n",
    "    cv = StratifiedKFold(n_splits=nsplit)\n",
    "    classes = np.unique(y)\n",
    "    colors = sns.color_palette('Dark2', params['Nclass'])\n",
    "    mean_r = np.linspace(0, 1, 100)\n",
    "    accuracy_tot = 0\n",
    "    nclass = len(classes)\n",
    "    for j in range(nclass):\n",
    "        ps = []\n",
    "        allAcc = []\n",
    "        aucs = []\n",
    "        all_confMatrices = []\n",
    "        for train, test in cv.split(X, y):\n",
    "            Xtrain_resampled = X[train]\n",
    "            ytrain_resampled = y[train]\n",
    "\n",
    "            probas_ = model.predict(X[test])#[0]\n",
    "            predictions = model.predict(X[test])#[0]\n",
    "            predictDF = pd.DataFrame(data=predictions, columns=classes)\n",
    "            predictions = predictDF.idxmax(axis=1)\n",
    "            precision, recall, thresholds = precision_recall_curve(y[test], probas_[:, j], pos_label=classes[j])\n",
    "            ps.append(interp(mean_r, recall[::-1], precision[::-1]))\n",
    "            pr_auc = auc(recall, precision)\n",
    "            aucs.append(pr_auc)\n",
    "            tempAccuracy =  np.sum(predictions == y[test])/len(y[test])*100\n",
    "            allAcc.append(tempAccuracy)\n",
    "            matr = confusion_matrix(y[test], predictions, normalize='true')\n",
    "            all_confMatrices.append(matr)\n",
    "            accuracy_tot += tempAccuracy\n",
    "        mean_p = np.mean(ps, axis=0)\n",
    "        mean_auc = auc(mean_r, mean_p)\n",
    "        std_auc = np.std(aucs)\n",
    "        accuracy = accuracy_tot / (nsplit*len(classes))\n",
    "        if std_auc < 0.01:\n",
    "            ax.plot(mean_r, mean_p, color=colors[j],\n",
    "                     label='%s (%0.2f $\\pm$ <0.01)' % (encoding_dict[j].replace(\"SN\", \"\"), mean_auc),\n",
    "                     lw=2, alpha=.8)\n",
    "        else:\n",
    "            ax.plot(mean_r, mean_p, color=colors[j],\n",
    "                     label='%s (%0.2f $\\pm$ %0.2f)' % (encoding_dict[j].replace(\"SN\", \"\"), mean_auc, std_auc),\n",
    "                     lw=2, alpha=.8)\n",
    "        std_p = np.std(ps, axis=0)\n",
    "        ps_upper = np.minimum(mean_p + std_p, 1)\n",
    "        ps_lower = np.maximum(mean_p - std_p, 0)\n",
    "        ax.fill_between(mean_r, ps_lower, ps_upper, color=colors[j], alpha=.3)\n",
    "\n",
    "    ax.set_xlabel(\"Precision\");\n",
    "    ax.set_ylabel(\"Recall\");\n",
    "    ax.legend()\n",
    "    #ax.legend(loc=4)\n",
    "    ax.set_xlim([-0.05, 1.05])\n",
    "    ax.set_ylim([-0.05, 1.05])\n",
    "    if save:\n",
    "        plt.savefig(plotpath + \"/Combined_MeanPR_Curve_%s.png\"% fnstr, dpi=150)\n",
    "    return accuracy, allAcc\n",
    "\n",
    "\n",
    "def plot_ROC_wCV(model, params, ax, X, y, encoding_dict, fnstr='', plotpath='./', save=True):\n",
    "    \"\"\"Short summary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : type\n",
    "        Description of parameter `model`.\n",
    "    ax : type\n",
    "        Description of parameter `ax`.\n",
    "    X : type\n",
    "        Description of parameter `X`.\n",
    "    y : type\n",
    "        Description of parameter `y`.\n",
    "    encoding_dict : type\n",
    "        Description of parameter `encoding_dict`.\n",
    "    fnstr : type\n",
    "        Description of parameter `fnstr`.\n",
    "    save : type\n",
    "        Description of parameter `save`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    type\n",
    "        Description of returned object.\n",
    "\n",
    "    \"\"\"\n",
    "    nsplit = 5\n",
    "    cv = StratifiedKFold(n_splits=nsplit)\n",
    "    classes = np.unique(y)\n",
    "    colors = sns.color_palette('Dark2', params['Nclass'])\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    accuracy_tot = 0\n",
    "    nclass = len(classes)\n",
    "    for j in range(nclass):\n",
    "        wrong = []\n",
    "        allRight = []\n",
    "        tprs = []\n",
    "        allAcc = []\n",
    "        aucs = []\n",
    "        all_confMatrices = []\n",
    "        for train, test in cv.split(X, y):\n",
    "            Xtrain_resampled = X[train]\n",
    "            ytrain_resampled = y[train]\n",
    "\n",
    "            probas_ = model.predict(X[test])#[0]\n",
    "            predictions = model.predict(X[test])\n",
    "            predictDF = pd.DataFrame(data=predictions, columns=classes)\n",
    "            predictions = predictDF.idxmax(axis=1)\n",
    "            fpr, tpr, thresholds = roc_curve(y[test], probas_[:, j], pos_label=classes[j])\n",
    "            tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            tempAccuracy =  np.sum(predictions == y[test])/len(y[test])*100\n",
    "            allAcc.append(tempAccuracy)\n",
    "            matr = confusion_matrix(y[test], predictions, normalize='true')\n",
    "            all_confMatrices.append(matr)\n",
    "            accuracy_tot += tempAccuracy\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        std_auc = np.std(aucs)\n",
    "        accuracy = accuracy_tot / (nsplit*len(classes))\n",
    "        if std_auc < 0.01:\n",
    "            ax.plot(mean_fpr, mean_tpr, color=colors[j],\n",
    "                     label='%s (%0.2f $\\pm$ <0.01)' % (encoding_dict[j].replace(\"SN\", \"\"), mean_auc),\n",
    "                     lw=2, alpha=.8)\n",
    "        else:\n",
    "            ax.plot(mean_fpr, mean_tpr, color=colors[j],\n",
    "                     label='%s (%0.2f $\\pm$ %0.2f)' % (encoding_dict[j].replace(\"SN\", \"\"), mean_auc, std_auc),\n",
    "                     lw=2, alpha=.8)\n",
    "        std_tpr = np.std(tprs, axis=0)\n",
    "        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "        ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color=colors[j], alpha=.3)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',alpha=.8)\n",
    "    ax.set_xlabel(\"False Positive Rate\");\n",
    "    ax.set_ylabel(\"True Positive Rate\");\n",
    "    ax.legend()\n",
    "    #ax.legend(loc=4)\n",
    "    ax.set_xlim([-0.05, 1.05])\n",
    "    ax.set_ylim([-0.05, 1.05])\n",
    "    if save:\n",
    "        plt.savefig(plotpath + \"/Combined_MeanROC_Curve_%s.png\"% fnstr,dpi=150)\n",
    "    return accuracy, allAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fe170-d6f4-4c48-af87-291b8ad18ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, confusion_matrix\n",
    "from numpy import interp\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "outdir = '/Users/alexgagliano/Documents/Research/HostClassifier/transformer/'\n",
    "params = {'Nclass':3}\n",
    "encoding_dict = {0:'SN II', 1:'SN Ia', 2:'SN Ibc'}\n",
    "\n",
    "######### ROC curves ################\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (8, 8))\n",
    "plot_ROC_wCV(model, params, fig.gca(), X_test_first3, y_test_first3, encoding_dict, fnstr=outputfn + \"_First3\", plotpath=outdir+'/plots/', save=True)\n",
    "\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (8, 8))\n",
    "plot_ROC_wCV(model, params, fig.gca(), X_test_mid15, y_test_mid15, encoding_dict, fnstr=outputfn + \"_Mid15\", plotpath=outdir+'/plots/', save=True)\n",
    "\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (8, 8))\n",
    "plot_ROC_wCV(model, params, fig.gca(), X_test_last15, y_test_last15, encoding_dict, fnstr=outputfn + \"_Last15\", plotpath=outdir+'/plots/', save=True)\n",
    "\n",
    "######### Precision-recall curves ################\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (8, 8))\n",
    "plot_PR_wCV(model, params, fig.gca(), X_test_first3, y_test_first3, encoding_dict, fnstr=outputfn + \"_First3\", plotpath=outdir+'/plots/', save=True)\n",
    "\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (8, 8))\n",
    "plot_PR_wCV(model, params, fig.gca(), X_test_mid15, y_test_mid15, encoding_dict, fnstr=outputfn + \"_Mid15\", plotpath=outdir+'/plots/', save=True)\n",
    "\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (8, 8))\n",
    "plot_PR_wCV(model, params, fig.gca(), X_test_last15, y_test_last15, encoding_dict, fnstr=outputfn + \"_Last15\", plotpath=outdir+'/plots/', save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iaifi_summerschool",
   "language": "python",
   "name": "iaifi_summerschool"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
